# scraper.py
import requests
import json
import time

def scrape_wikipedia_malagasy(num_pages=50):
    """Scrape Wikipedia Malagasy"""
    
    print("üîÑ Scraping Wikipedia Malagasy...")
    
    session = requests.Session()
    session.headers.update({
        'User-Agent': 'MalagasyProject/1.0'
    })
    
    articles = []
    api_url = "https://mg.wikipedia.org/w/api.php"
    
    # √âtape 1: R√©cup√©rer liste de pages al√©atoires
    params = {
        "action": "query",
        "format": "json",
        "list": "random",
        "rnlimit": num_pages,
        "rnnamespace": 0
    }
    
    try:
        resp = session.get(api_url, params=params, timeout=15, verify=False)
        pages = resp.json()["query"]["random"]
        print(f"‚úì {len(pages)} pages trouv√©es")
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
        return []
    
    # √âtape 2: R√©cup√©rer contenu de chaque page
    for i, page in enumerate(pages):
        try:
            params = {
                "action": "query",
                "format": "json",
                "pageids": page["id"],
                "prop": "extracts",
                "explaintext": True
            }
            
            resp = session.get(api_url, params=params, timeout=10, verify=False)
            data = resp.json()["query"]["pages"][str(page["id"])]
            
            content = data.get("extract", "")
            
            if content and len(content) > 50:
                articles.append({
                    "id": page["id"],
                    "title": page["title"],
                    "content": content
                })
                print(f"  [{i+1}/{len(pages)}] ‚úì {page['title'][:50]}")
            
            time.sleep(0.2)
            
        except Exception as e:
            print(f"  [{i+1}/{len(pages)}] ‚úó Erreur: {page['title']}")
            continue
    
    return articles


def save_articles(articles, filename="articles_raw.json"):
    """Sauvegarde les articles"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    print(f"\nüíæ Sauvegard√©: {filename} ({len(articles)} articles)")


if __name__ == "__main__":
    # Lancer le scraping
    articles = scrape_wikipedia_malagasy(50)
    
    if articles:
        save_articles(articles)
        print(f"\n‚úÖ Termin√©! {len(articles)} articles r√©cup√©r√©s")
    else:
        print("\n‚ùå Aucun article r√©cup√©r√©")