# cleaner.py
import json
import re
from collections import Counter

def load_articles(filename="articles_raw.json"):
    """Charge les articles scrapÃ©s"""
    with open(filename, 'r', encoding='utf-8') as f:
        return json.load(f)


def clean_text(text):
    """Nettoie un texte"""
    if not text:
        return ""
    
    # Supprimer rÃ©fÃ©rences [1], [2]...
    text = re.sub(r'\[\d+\]', '', text)
    
    # Supprimer URLs
    text = re.sub(r'https?://\S+', '', text)
    
    # Supprimer caractÃ¨res spÃ©ciaux (garder ponctuation de base)
    text = re.sub(r'[^\w\s\.,;:!?\'\"-]', ' ', text)
    
    # Normaliser espaces
    text = re.sub(r'\s+', ' ', text)
    
    return text.strip()


def extract_words(text):
    """Extrait les mots d'un texte"""
    text = clean_text(text).lower()
    words = re.findall(r'\b[a-zA-Z]{2,}\b', text)
    return words


def extract_sentences(text):
    """Extrait les phrases d'un texte"""
    text = clean_text(text)
    sentences = re.split(r'[.!?]+', text)
    # Garder phrases avec au moins 3 mots
    return [s.strip() for s in sentences if len(s.strip().split()) >= 3]


def detect_malagasy_quality(text):
    """DÃ©tecte si le texte est bien en malagasy (score 0-1)"""
    # Mots trÃ¨s courants en malagasy
    common_mg = {'ny', 'sy', 'dia', 'ary', 'fa', 'izany', 'izy', 'amin', 
                 'ho', 'tsy', 'na', 'ao', 'an', 'eo', 'io', 'no', 'mba'}
    
    words = extract_words(text)
    if not words:
        return 0
    
    mg_count = sum(1 for w in words if w in common_mg)
    return mg_count / len(words)


def process_articles(articles):
    """Traite tous les articles"""
    
    print("ðŸ§¹ Nettoyage des articles...")
    
    clean_articles = []
    all_words = []
    all_sentences = []
    
    for article in articles:
        content = article.get("content", "")
        
        # VÃ©rifier qualitÃ© malagasy
        quality = detect_malagasy_quality(content)
        
        # Nettoyer
        clean_content = clean_text(content)
        words = extract_words(content)
        sentences = extract_sentences(content)
        
        # Stocker
        clean_articles.append({
            "id": article.get("id"),
            "title": article["title"],
            "content_clean": clean_content,
            "word_count": len(words),
            "sentence_count": len(sentences),
            "quality_score": round(quality, 3)
        })
        
        all_words.extend(words)
        all_sentences.extend(sentences)
        
        # Afficher statut
        status = "âœ“" if quality > 0.03 else "âš "
        print(f"  {status} {article['title'][:40]:<40} | {len(words):>4} mots | qualitÃ©: {quality:.1%}")
    
    return clean_articles, all_words, all_sentences


def build_word_frequencies(words):
    """Compte frÃ©quence des mots"""
    return dict(Counter(words).most_common())


def save_json(data, filename):
    """Sauvegarde en JSON"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"ðŸ’¾ {filename}")


def save_text(lines, filename):
    """Sauvegarde en texte"""
    with open(filename, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
    print(f"ðŸ’¾ {filename}")


if __name__ == "__main__":
    
    # Charger articles
    print("ðŸ“‚ Chargement articles_raw.json...")
    articles = load_articles("articles_raw.json")
    print(f"   {len(articles)} articles chargÃ©s\n")
    
    # Traiter
    clean_articles, all_words, all_sentences = process_articles(articles)
    
    # Stats
    word_freq = build_word_frequencies(all_words)
    unique_words = list(word_freq.keys())
    
    print(f"\nðŸ“Š Statistiques:")
    print(f"   Articles: {len(clean_articles)}")
    print(f"   Mots totaux: {len(all_words)}")
    print(f"   Mots uniques: {len(unique_words)}")
    print(f"   Phrases: {len(all_sentences)}")
    
    # Top 20 mots
    print(f"\nðŸ“ˆ Top 20 mots:")
    for word, count in list(word_freq.items())[:20]:
        print(f"   {word:<15} {count}")
    
    # Sauvegarder
    print(f"\nðŸ’¾ Sauvegarde...")
    save_json(clean_articles, "articles_clean.json")
    save_json(word_freq, "word_frequencies.json")
    save_json(unique_words, "dictionnaire_mg.json")
    save_text(all_sentences, "sentences.txt")
    
    print(f"\nâœ… Nettoyage terminÃ©!")